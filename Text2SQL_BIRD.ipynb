{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nQs7YNktwvc"
   },
   "source": [
    "https://stackoverflow.com/questions/56449262/how-to-upload-folders-to-google-colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5gD_DHNt4Fe"
   },
   "source": [
    "https://medium.com/dataherald/uploading-bird-nl-to-sql-data-to-bigquery-91ce6af7fa5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "XeSsQ_NQ_Dnd",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "e1c36aa6-a2b1-4488-8a7c-7559047e52e8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: anthropic in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (0.30.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from anthropic) (2.8.2)\n",
      "Requirement already satisfied: tokenizers>=0.13.0 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from anthropic) (0.19.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from anthropic) (0.5.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from anthropic) (1.3.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from anthropic) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from anthropic) (1.9.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from anthropic) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from anthropic) (0.27.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from anyio<5,>=3.5.0->anthropic) (1.2.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from anyio<5,>=3.5.0->anthropic) (3.7)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from httpx<1,>=0.23.0->anthropic) (1.0.5)\n",
      "Requirement already satisfied: certifi in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from httpx<1,>=0.23.0->anthropic) (2024.7.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic) (0.14.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from pydantic<3,>=1.9.0->anthropic) (2.20.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from tokenizers>=0.13.0->anthropic) (0.23.4)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (24.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2024.6.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.15.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (4.66.4)\n",
      "Requirement already satisfied: requests in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.32.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (6.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (0.4.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.3.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ai21>=2.2.0 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (2.9.0)\n",
      "Collecting ai21>=2.2.0\n",
      "  Downloading ai21-2.9.1-py3-none-any.whl (75 kB)\n",
      "     -------------------------------------- 75.1/75.1 kB 463.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.9.0 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from ai21>=2.2.0) (4.12.2)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from ai21>=2.2.0) (0.27.0)\n",
      "Requirement already satisfied: ai21-tokenizer<1.0.0,>=0.11.0 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from ai21>=2.2.0) (0.11.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.3 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from ai21>=2.2.0) (0.6.7)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.3.0 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from ai21>=2.2.0) (8.5.0)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.4.0 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from ai21-tokenizer<1.0.0,>=0.11.0->ai21>=2.2.0) (4.4.0)\n",
      "Requirement already satisfied: sentencepiece<1.0.0,>=0.2.0 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from ai21-tokenizer<1.0.0,>=0.11.0->ai21>=2.2.0) (0.2.0)\n",
      "Requirement already satisfied: tokenizers<1.0.0,>=0.15.0 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from ai21-tokenizer<1.0.0,>=0.11.0->ai21>=2.2.0) (0.19.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.3->ai21>=2.2.0) (0.9.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.3->ai21>=2.2.0) (3.21.3)\n",
      "Requirement already satisfied: idna in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ai21>=2.2.0) (3.7)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ai21>=2.2.0) (1.0.5)\n",
      "Requirement already satisfied: certifi in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ai21>=2.2.0) (2024.7.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ai21>=2.2.0) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ai21>=2.2.0) (0.14.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from anyio<5.0.0,>=4.4.0->ai21-tokenizer<1.0.0,>=0.11.0->ai21>=2.2.0) (1.2.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7.0,>=0.6.3->ai21>=2.2.0) (24.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from tokenizers<1.0.0,>=0.15.0->ai21-tokenizer<1.0.0,>=0.11.0->ai21>=2.2.0) (0.23.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.3->ai21>=2.2.0) (1.0.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1.0.0,>=0.15.0->ai21-tokenizer<1.0.0,>=0.11.0->ai21>=2.2.0) (4.66.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1.0.0,>=0.15.0->ai21-tokenizer<1.0.0,>=0.11.0->ai21>=2.2.0) (3.15.4)\n",
      "Requirement already satisfied: requests in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1.0.0,>=0.15.0->ai21-tokenizer<1.0.0,>=0.11.0->ai21>=2.2.0) (2.32.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1.0.0,>=0.15.0->ai21-tokenizer<1.0.0,>=0.11.0->ai21>=2.2.0) (2024.6.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1.0.0,>=0.15.0->ai21-tokenizer<1.0.0,>=0.11.0->ai21>=2.2.0) (6.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0,>=0.16.4->tokenizers<1.0.0,>=0.15.0->ai21-tokenizer<1.0.0,>=0.11.0->ai21>=2.2.0) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers<1.0.0,>=0.15.0->ai21-tokenizer<1.0.0,>=0.11.0->ai21>=2.2.0) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers<1.0.0,>=0.15.0->ai21-tokenizer<1.0.0,>=0.11.0->ai21>=2.2.0) (2.2.2)\n",
      "Installing collected packages: ai21\n",
      "  Attempting uninstall: ai21\n",
      "    Found existing installation: ai21 2.9.0\n",
      "    Uninstalling ai21-2.9.0:\n",
      "      Successfully uninstalled ai21-2.9.0\n",
      "Successfully installed ai21-2.9.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (4.42.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: requests in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from requests->transformers) (2.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (2.8.2)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from pydantic) (2.20.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\cooli\\pycharmprojects\\anlp_text2sql_bird\\.env\\lib\\site-packages (from pydantic) (4.12.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install anthropic\n",
    "!pip install -U \"ai21>=2.2.0\"\n",
    "!pip install transformers\n",
    "!pip install pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZpkTLpOZsZL-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\97252\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "import pickle\n",
    "import transformers\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "import Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "YfagsSd7M7Xm"
   },
   "outputs": [],
   "source": [
    "schema_A_info_file_path=\"./data/schema_A.json\"\n",
    "schema_D_info_file_path=\"./data/schema_D.json\"\n",
    "table_info_file_path=\"./data/table_info.json\"\n",
    "dev_table_info_file_path=\"./data/dev_tables.json\"\n",
    "dev_file_path=\"dev.json\"\n",
    "databases_to_check = ['debit_card_specializing', 'financial', 'formula_1', 'student_club', 'superhero', 'thrombosis_prediction', 'toxicology']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cooli\\PycharmProjects\\anlp_text2sql_bird\\.env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "sqlcoder_pipe = pipeline(\"text-generation\", model=\"defog/sqlcoder-7b-2\", device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true,
    "id": "v7XC_qdL-4QT",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "def call_claude(prompt: str) -> str:\n",
    "    # Initialize the client\n",
    "    client = anthropic.Anthropic(\n",
    "        # defaults to os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "        api_key=\"<API KEY>\",\n",
    "    )\n",
    "\n",
    "    # Call the API\n",
    "    message = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20240620\",\n",
    "        max_tokens=1000,\n",
    "        temperature=0,\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": prompt}]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(message.content)\n",
    "    return message.completion\n",
    "\n",
    "\n",
    "from ai21 import AI21Client\n",
    "from ai21.models.chat import ChatMessage\n",
    "\n",
    "def call_a21(prompt: str) -> str:\n",
    "    client = AI21Client(api_key=\"<API KEY>\")\n",
    "    messages = [\n",
    "        ChatMessage(\n",
    "            role=\"user\",\n",
    "            content=prompt\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"jamba-instruct-preview\",\n",
    "        messages=messages,\n",
    "        top_p=0.1 # Setting to 1 encourages different responses each call.\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content, response.usage\n",
    "\n",
    "\n",
    "def call_sqlcoder(prompt: str) -> str:\n",
    "    # Use a pipeline as a high-level helper\n",
    "    sqlcoder_pipe.model.to(\"cuda:0\")\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    response = sqlcoder_pipe(messages, max_new_tokens=200, num_beams=4, do_sample=False)\n",
    "    return response[0]['generated_text'][1]['content'], None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdQuestion(BaseModel):\n",
    "    question_id: int\n",
    "    db_id: str\n",
    "    question: str\n",
    "    evidence: str\n",
    "    SQL: str\n",
    "    difficulty: str\n",
    "\n",
    "\n",
    "# tables_info - contains the tables in the dataset, and the columns of each table\n",
    "def get_table_columns(table_name: str, tables_info: dict) -> List[str]:\n",
    "    curent_table_info = tables_info[table_name]\n",
    "    columns = []\n",
    "    for key in curent_table_info.keys():\n",
    "        if key == 'tables':\n",
    "            continue\n",
    "        columns.extend(curent_table_info[key])\n",
    "    raise columns\n",
    "\n",
    "\n",
    "def extract_tables_from_response(dataset_name:str, response: str, tables_info: dict) -> List[str]:\n",
    "    dataset_tables = tables_info[dataset_name]['tables']\n",
    "    extracted_tables = []\n",
    "    for table_name in dataset_tables:\n",
    "        if table_name in response:\n",
    "            extracted_tables.append(table_name)\n",
    "    return extracted_tables\n",
    "\n",
    "\n",
    "def extract_columns_from_response(dataset_name:str, response: str, tables_info: dict, relevant_tables: list[str]) -> List[str]:\n",
    "    dataset_info = tables_info[dataset_name]\n",
    "    extracted_columns = []\n",
    "    for table_name in relevant_tables:\n",
    "        table_columns = dataset_info[table_name]\n",
    "        for column_name in table_columns:\n",
    "            if column_name in response:\n",
    "                extracted_columns.append(f\"{table_name}.{column_name}\")\n",
    "    return extracted_columns\n",
    "\n",
    "\n",
    "# Schema_X_info - Information about all the dataset tables. contains dataset name, table names, and columns data\n",
    "# Schema_A_info - ALL data (Type, Description, Values, IS_PRIMARY_KEY), Schema_D_info - Only description data\n",
    "def generate_schema_D(question: BirdQuestion) -> str:\n",
    "    schema_info = json.load(open(schema_D_info_file_path, 'r'))\n",
    "\n",
    "    schema_str = \"\"\n",
    "    schema_foriegn_keys = schema_info[question.db_id]['foreign_keys']\n",
    "    for table_name, columns in schema_info[question.db_id].items():\n",
    "        if table_name == 'foreign_keys':\n",
    "            continue\n",
    "        schema_str += table_name + \" (\\n\"\n",
    "        for column_name, column_info in columns.items():\n",
    "            schema_str += column_name + \": \" + column_info + \"\\n\"\n",
    "\n",
    "        schema_str.rstrip(\"\\n\")\n",
    "        schema_str += \")\\n\"\n",
    "\n",
    "    schema_str += \"FOREIGN KEYS:\\n\"\n",
    "    for foreign_key in schema_foriegn_keys:\n",
    "        schema_str += foreign_key + \"\\n\"\n",
    "\n",
    "    schema_str.rstrip(\"\\n\")\n",
    "\n",
    "    return schema_str\n",
    "\n",
    "\n",
    "def generate_schema_A_all_tables(question: BirdQuestion) -> str:\n",
    "    schema_info = json.load(open(schema_A_info_file_path, 'r'))\n",
    "\n",
    "    schema_str = \"\"\n",
    "    schema_foriegn_keys = schema_info[question.db_id]['foreign_keys']\n",
    "    for table_name, columns in schema_info[question.db_id].items():\n",
    "        if table_name == 'foreign_keys':\n",
    "            continue\n",
    "        schema_str += table_name + \" (\\n\"\n",
    "        for column_name, column_info in columns.items():\n",
    "            schema_str += column_name + \": \" + column_info + \"\\n\"\n",
    "\n",
    "        schema_str.rstrip(\"\\n\")\n",
    "        schema_str += \")\\n\"\n",
    "\n",
    "    schema_str += \"FOREIGN KEYS:\\n\"\n",
    "    for foreign_key in schema_foriegn_keys:\n",
    "        schema_str += foreign_key + \"\\n\"\n",
    "\n",
    "    schema_str.rstrip(\"\\n\")\n",
    "\n",
    "    return schema_str\n",
    "\n",
    "\n",
    "def generate_schema_A(question: BirdQuestion, predicted_tables: List[str]) -> str:\n",
    "    schema_info = json.load(open(schema_A_info_file_path, 'r'))\n",
    "\n",
    "    schema_str = \"\"\n",
    "    schema_foriegn_keys = schema_info[question.db_id]['foreign_keys']\n",
    "    for table_name, columns in schema_info[question.db_id].items():\n",
    "        if table_name == 'foreign_keys' or table_name not in predicted_tables:\n",
    "            continue\n",
    "        schema_str += table_name + \" (\\n\"\n",
    "        for column_name, column_info in columns.items():\n",
    "            schema_str += column_name + \": \" + column_info + \"\\n\"\n",
    "\n",
    "        schema_str.rstrip(\"\\n\")\n",
    "        schema_str += \")\\n\"\n",
    "\n",
    "    schema_str += \"FOREIGN KEYS:\\n\"\n",
    "    for foreign_key in schema_foriegn_keys:\n",
    "        key1, key2 = foreign_key.split('=')\n",
    "        table_of_key1 = key1.split('.')[0]\n",
    "        table_of_key2 = key2.split('.')[0]\n",
    "        if table_of_key1 in predicted_tables and table_of_key2 in predicted_tables:\n",
    "            schema_str += foreign_key + \"\\n\"\n",
    "\n",
    "    schema_str.rstrip(\"\\n\")\n",
    "\n",
    "    return schema_str\n",
    "\n",
    "\n",
    "def generate_schema_N_all_tables(question: BirdQuestion) -> str:\n",
    "    schema_info = json.load(open(schema_D_info_file_path, 'r'))\n",
    "\n",
    "    schema_str = \"\"\n",
    "    schema_foriegn_keys = schema_info[question.db_id]['foreign_keys']\n",
    "    for table_name, columns in schema_info[question.db_id].items():\n",
    "        if table_name == 'foreign_keys':\n",
    "            continue\n",
    "        schema_str += table_name + \" (\\n\"\n",
    "        for column_name, column_info in columns.items():\n",
    "            schema_str += column_name + \"\\n\"\n",
    "\n",
    "        schema_str.rstrip(\"\\n\")\n",
    "        schema_str += \")\\n\"\n",
    "\n",
    "    schema_str += \"FOREIGN KEYS:\\n\"\n",
    "    for foreign_key in schema_foriegn_keys:\n",
    "        schema_str += foreign_key + \"\\n\"\n",
    "\n",
    "    schema_str.rstrip(\"\\n\")\n",
    "\n",
    "    return schema_str\n",
    "\n",
    "\n",
    "def generate_schema_N(question: BirdQuestion, predicted_tables: List[str]) -> str:\n",
    "    schema_info = json.load(open(schema_D_info_file_path, 'r'))\n",
    "\n",
    "    schema_str = \"\"\n",
    "    schema_foriegn_keys = schema_info[question.db_id]['foreign_keys']\n",
    "    for table_name, columns in schema_info[question.db_id].items():\n",
    "        if table_name == 'foreign_keys' or table_name not in predicted_tables:\n",
    "            continue\n",
    "        schema_str += table_name + \" (\\n\"\n",
    "        for column_name, column_info in columns.items():\n",
    "            schema_str += column_name + \"\\n\"\n",
    "\n",
    "        schema_str.rstrip(\"\\n\")\n",
    "        schema_str += \")\\n\"\n",
    "\n",
    "    schema_str += \"FOREIGN KEYS:\\n\"\n",
    "    for foreign_key in schema_foriegn_keys:\n",
    "        key1, key2 = foreign_key.split('=')\n",
    "        table_of_key1 = key1.split('.')[0]\n",
    "        table_of_key2 = key2.split('.')[0]\n",
    "        if table_of_key1 in predicted_tables and table_of_key2 in predicted_tables:\n",
    "            schema_str += foreign_key + \"\\n\"\n",
    "\n",
    "    schema_str.rstrip(\"\\n\")\n",
    "\n",
    "    return schema_str\n",
    "\n",
    "\n",
    "def generate_schema_VDT_all_tables(question: BirdQuestion) -> str:\n",
    "    schema_info = json.load(open(schema_A_info_file_path, 'r'))\n",
    "\n",
    "    schema_str = \"\"\n",
    "    schema_foriegn_keys = schema_info[question.db_id]['foreign_keys']\n",
    "    for table_name, columns in schema_info[question.db_id].items():\n",
    "        if table_name == 'foreign_keys':\n",
    "            continue\n",
    "        schema_str += table_name + \" (\\n\"\n",
    "        for column_name, column_info in columns.items():\n",
    "            schema_str += column_name + \": \" + column_info.replace(', PRIMARY KEY', '') + \"\\n\"\n",
    "            \n",
    "        schema_str.rstrip(\"\\n\")\n",
    "        schema_str += \")\\n\"\n",
    "\n",
    "    schema_str += \"FOREIGN KEYS:\\n\"\n",
    "    for foreign_key in schema_foriegn_keys:\n",
    "        schema_str += foreign_key + \"\\n\"\n",
    "\n",
    "    schema_str.rstrip(\"\\n\")\n",
    "\n",
    "    return schema_str\n",
    "\n",
    "\n",
    "def generate_schema_VDT(question: BirdQuestion, predicted_tables: List[str]) -> str:\n",
    "    schema_info = json.load(open(schema_A_info_file_path, 'r'))\n",
    "\n",
    "    schema_str = \"\"\n",
    "    schema_foriegn_keys = schema_info[question.db_id]['foreign_keys']\n",
    "    for table_name, columns in schema_info[question.db_id].items():\n",
    "        if table_name == 'foreign_keys' or table_name not in predicted_tables:\n",
    "            continue\n",
    "        schema_str += table_name + \" (\\n\"\n",
    "        for column_name, column_info in columns.items():\n",
    "            schema_str += column_name + \": \" + column_info.replace(', PRIMARY KEY', '') + \"\\n\"\n",
    "\n",
    "        schema_str.rstrip(\"\\n\")\n",
    "        schema_str += \")\\n\"\n",
    "\n",
    "    schema_str += \"FOREIGN KEYS:\\n\"\n",
    "    for foreign_key in schema_foriegn_keys:\n",
    "        key1, key2 = foreign_key.split('=')\n",
    "        table_of_key1 = key1.split('.')[0]\n",
    "        table_of_key2 = key2.split('.')[0]\n",
    "        if table_of_key1 in predicted_tables and table_of_key2 in predicted_tables:\n",
    "            schema_str += foreign_key + \"\\n\"\n",
    "\n",
    "    schema_str.rstrip(\"\\n\")\n",
    "\n",
    "    return schema_str\n",
    "\n",
    "\n",
    "def get_predicted_sql_query(question: BirdQuestion, method: str = \"ALL\", model: str = \"A21\") -> str:\n",
    "    schema_d_info = generate_schema_D(question)\n",
    "    tables_info = json.load(open(table_info_file_path, 'r'))\n",
    "\n",
    "    # print(\"======================= STEP 1 =======================\")\n",
    "    # Step 1 - Extract tables\n",
    "    predict_tables_prompt = Prompts.PREDICT_TABLES_PROMPT_DESCRIPTION.format(table_info=schema_d_info,\n",
    "                                                                     question=question.question,\n",
    "                                                                     note=question.evidence)\n",
    "    \n",
    "    if model == \"A21\":\n",
    "        response, usage = call_a21(predict_tables_prompt)\n",
    "    elif model == \"SQLCODER\":\n",
    "        response, usage = call_sqlcoder(predict_tables_prompt)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model\")\n",
    "    \n",
    "    extracted_tables = extract_tables_from_response(question.db_id, response, tables_info)\n",
    "\n",
    "    schema_a_info = generate_schema_A(question, extracted_tables)\n",
    "    # print(\"======================= STEP 2 =======================\")\n",
    "    # Step 2 - Extract columns\n",
    "    if method == \"ALL\":\n",
    "        predict_columns_prompt = Prompts.PREDICT_COLUMNS_PROMPT_ALL.format(table_info=generate_schema_A(question, extracted_tables),\n",
    "                                                                question=question.question,\n",
    "                                                                note=question.evidence,\n",
    "                                                                used_tables= ', '.join(extracted_tables))\n",
    "    elif method == \"VDT\":\n",
    "        predict_columns_prompt = Prompts.PREDICT_COLUMNS_PROMPT_VDT.format(table_info=generate_schema_VDT(question, extracted_tables),\n",
    "                                                                question=question.question,\n",
    "                                                                note=question.evidence,\n",
    "                                                                used_tables= ', '.join(extracted_tables))\n",
    "    elif method == \"N\":\n",
    "        predict_columns_prompt = Prompts.PREDICT_COLUMNS_PROMPT_N.format(table_info=generate_schema_N(question, extracted_tables),\n",
    "                                                                question=question.question,\n",
    "                                                                note=question.evidence,\n",
    "                                                                used_tables= ', '.join(extracted_tables))\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method\")\n",
    "\n",
    "\n",
    "    if model == \"A21\":\n",
    "        response, usage = call_a21(predict_columns_prompt)\n",
    "    elif model == \"SQLCODER\":\n",
    "        response, usage = call_sqlcoder(predict_columns_prompt)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model\")\n",
    "        \n",
    "    extracted_columns = extract_columns_from_response(question.db_id, response, tables_info, extracted_tables)\n",
    "\n",
    "    # print(\"======================= STEP 3 =======================\")\n",
    "    # Step 3 - Generate SQL query\n",
    "    if method == \"ALL\":\n",
    "        predict_sql_query_prompt = Prompts.PREDICT_SQL_PROMPT_ALL.format(table_info=generate_schema_A(question, extracted_tables),\n",
    "                                                            question=question.question,\n",
    "                                                            note=question.evidence,\n",
    "                                                            used_tables_and_columns= ', '.join(extracted_columns))\n",
    "    elif method == \"VDT\":\n",
    "        predict_sql_query_prompt = Prompts.PREDICT_SQL_PROMPT_VDT.format(table_info=generate_schema_VDT(question, extracted_tables),\n",
    "                                                            question=question.question,\n",
    "                                                            note=question.evidence,\n",
    "                                                            used_tables_and_columns= ', '.join(extracted_columns))\n",
    "    elif method == \"N\":\n",
    "        predict_sql_query_prompt = Prompts.PREDICT_SQL_PROMPT_N.format(table_info=generate_schema_N(question, extracted_tables),\n",
    "                                                            question=question.question,\n",
    "                                                            note=question.evidence,\n",
    "                                                            used_tables_and_columns= ', '.join(extracted_columns))\n",
    "\n",
    "    if model == \"A21\":\n",
    "        response, usage = call_a21(predict_sql_query_prompt)\n",
    "    elif model == \"SQLCODER\":\n",
    "        response, usage = call_sqlcoder(predict_sql_query_prompt)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model\")\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_predicted_sql_query_no_COT(question: BirdQuestion, method: str = \"ALL\", model: str = \"A21\") -> str:\n",
    "    # ========================== CHANGE THIS ==========================\n",
    "    schema_d_info = generate_schema_D(question)\n",
    "    tables_info = json.load(open(table_info_file_path, 'r'))\n",
    "\n",
    "    # Generate SQL query\n",
    "    if method == \"ALL\":\n",
    "        predict_sql_query_prompt = Prompts.PREDICT_SQL_NO_COT_PROMPT_ALL.format(table_info=generate_schema_A_all_tables(question),\n",
    "                                                            question=question.question,\n",
    "                                                            note=question.evidence)\n",
    "    elif method == \"VDT\":\n",
    "        predict_sql_query_prompt = Prompts.PREDICT_SQL_NO_COT_PROMPT_VDT.format(table_info=generate_schema_VDT_all_tables(question),\n",
    "                                                            question=question.question,\n",
    "                                                            note=question.evidence)\n",
    "    elif method == \"N\":\n",
    "        predict_sql_query_prompt = Prompts.PREDICT_SQL_NO_COT_PROMPT_N.format(table_info=generate_schema_N_all_tables(question),\n",
    "                                                            question=question.question,\n",
    "                                                            note=question.evidence)\n",
    "\n",
    "    if model == \"A21\":\n",
    "        response, usage = call_a21(predict_sql_query_prompt)\n",
    "    elif model == \"SQLCODER\":\n",
    "        response, usage = call_sqlcoder(predict_sql_query_prompt)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model\")\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_predicted_sql_query_via_sqlcoder_prompt(question: BirdQuestion):\n",
    "    predict_sql_query_prompt = Prompts.PREDICT_SQLCODER_PROMPT.format(table_metadata_string_DDL_statements=generate_schema_A_all_tables(question),\n",
    "                                                                      user_question=question.question,\n",
    "                                                                      external_knowledge=question.evidence)\n",
    "\n",
    "    response, usage = call_sqlcoder(predict_sql_query_prompt)\n",
    "    return response\n",
    "\n",
    "\n",
    "def compare_sql_queries(question: BirdQuestion, predicted_query: str) -> bool:\n",
    "    # Open a connection to the SQLite database file\n",
    "    sqlite_database = f\"dev_datasets/{question.db_id}.sqlite\"\n",
    "    # print(sqlite_database)\n",
    "    con = sqlite3.connect(sqlite_database)\n",
    "    cur = con.cursor()\n",
    "\n",
    "    # Run predicted SQL query\n",
    "    try:\n",
    "        cur.execute(predicted_query)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False, e\n",
    "    predicted_query_results = cur.fetchall()\n",
    "    # print(\"Predicted SQL query:\")\n",
    "    # for row in predicted_query_results:\n",
    "    #     print(row)\n",
    "\n",
    "    # Run golden SQL query\n",
    "    cur.execute(question.SQL)\n",
    "    golden_query_results = cur.fetchall()\n",
    "    # print(\"Golden SQL query:\")\n",
    "    # for row in golden_query_results:\n",
    "    #     print(row)\n",
    "\n",
    "    con.close()\n",
    "\n",
    "    # Compare the results:\n",
    "    if len(predicted_query_results) != len(golden_query_results):\n",
    "        return False, None\n",
    "\n",
    "    for i in range(len(predicted_query_results)):\n",
    "        if predicted_query_results[i] != golden_query_results[i]:\n",
    "            return False, None\n",
    "\n",
    "    return True, None\n",
    "\n",
    "\n",
    "def process_dev_data(method:str, cot:bool, model:str, dev_file_path:str = dev_file_path, num:int = 300) ->list[dict]:\n",
    "    results = []\n",
    "    dev = json.load(open(dev_file_path, 'r'))\n",
    "    relevate_dev_dataset = dev\n",
    "    # Filter out 'big' databases\n",
    "    # relevate_dev_dataset = [question for question in dev if question['db_id'] in databases_to_check]\n",
    "    if cot:\n",
    "        file_name = f'{model}_results_cot_{method.lower()}_predict.pkl'\n",
    "    else:\n",
    "        file_name = f'{model}_results_no_cot_{method.lower()}_predict.pkl'\n",
    "        \n",
    "    for i, question in enumerate(relevate_dev_dataset[:num]):\n",
    "\n",
    "        question = BirdQuestion(**question)\n",
    "        if cot:\n",
    "            predicted_sql = get_predicted_sql_query(question, method, model)\n",
    "        else:\n",
    "            predicted_sql = get_predicted_sql_query_no_COT(question, method, model)\n",
    "            \n",
    "        compare_response = compare_sql_queries(question, predicted_sql)\n",
    "        results.append({'question': question,\n",
    "                        'success_status': compare_response[0],\n",
    "                        'reason': compare_response[1]})\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(i)\n",
    "            # Pickle the object\n",
    "            with open(file_name, 'wb') as file:\n",
    "                pickle.dump(results, file)\n",
    "\n",
    "    with open(file_name, 'wb') as file:\n",
    "        pickle.dump(results, file)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load one question to observe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9xrfeLr-5oqQ",
    "outputId": "c8ac4d91-85e1-4163-b651-25a78a39f61d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_id=100 db_id='financial' question='Among the account opened, how many female customers who were born before 1950 and stayed in Sokolov?' evidence=\"Customers refer to clients; Female refers to gender = 'F'; Names of districts appear in column A2\" SQL=\"SELECT COUNT(T2.client_id) FROM district AS T1 INNER JOIN client AS T2 ON T1.district_id = T2.district_id WHERE T2.gender = 'F' AND STRFTIME('%Y', T2.birth_date) < '1950' AND T1.A2 = 'Sokolov'\" difficulty='moderate'\n"
     ]
    }
   ],
   "source": [
    "dev = json.load(open(dev_file_path, 'r'))\n",
    "question = BirdQuestion(**dev[100])\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y9MbWx2O6UPL",
    "outputId": "cc97a00c-1079-4db1-a643-64bb11606aed"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sqlcoder_pipe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mget_predicted_sql_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSQLCODER\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 236\u001b[0m, in \u001b[0;36mget_predicted_sql_query\u001b[1;34m(question, method, model)\u001b[0m\n\u001b[0;32m    234\u001b[0m     response, usage \u001b[38;5;241m=\u001b[39m call_a21(predict_tables_prompt)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSQLCODER\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 236\u001b[0m     response, usage \u001b[38;5;241m=\u001b[39m \u001b[43mcall_sqlcoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredict_tables_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 49\u001b[0m, in \u001b[0;36mcall_sqlcoder\u001b[1;34m(prompt)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_sqlcoder\u001b[39m(prompt: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# Use a pipeline as a high-level helper\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m     \u001b[43msqlcoder_pipe\u001b[49m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     50\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     51\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt},\n\u001b[0;32m     52\u001b[0m     ]\n\u001b[0;32m     54\u001b[0m     response \u001b[38;5;241m=\u001b[39m sqlcoder_pipe(messages, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, num_beams\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sqlcoder_pipe' is not defined"
     ]
    }
   ],
   "source": [
    "response = get_predicted_sql_query(question, model=\"SQLCODER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SELECT COUNT(DISTINCT c.client_id) FROM account a JOIN client c ON a.district_id = c.district_id JOIN district d ON a.district_id = d.district_id WHERE c.gender = 'F' AND c.birth_date < '1950-01-01' AND d.A2 = 'Sokolov';\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check dataset analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 1534\n",
      "% Simple: 0.6029986962190352\n",
      "% Moderate: 0.30247718383311606\n",
      "% Challenging: 0.09452411994784876\n"
     ]
    }
   ],
   "source": [
    "dev = json.load(open(dev_file_path, 'r'))\n",
    "# relevate_dev_dataset = [question for question in dev if question['db_id'] in databases_to_check]\n",
    "question_obj_list = [BirdQuestion(**question) for question in dev]\n",
    "\n",
    "print(len(question_obj_list))\n",
    "\n",
    "datasets = {}\n",
    "for question in question_obj_list:\n",
    "    if question.db_id not in datasets:\n",
    "        datasets[question.db_id] = {}\n",
    "\n",
    "    if question.difficulty not in datasets[question.db_id]:\n",
    "        datasets[question.db_id][question.difficulty] = 0\n",
    "    if question.difficulty in datasets[question.db_id]:\n",
    "        datasets[question.db_id][question.difficulty] += 1\n",
    "    \n",
    "simple = []\n",
    "moderate = []\n",
    "challenging = []\n",
    "for dataset, value in datasets.items():\n",
    "    for difficulty, count in value.items():\n",
    "        if difficulty == 'simple':\n",
    "            simple.append(count)\n",
    "        elif difficulty == 'moderate':\n",
    "            moderate.append(count)\n",
    "        elif difficulty == 'challenging':\n",
    "            challenging.append(count)\n",
    "\n",
    "total_records = sum(simple + moderate + challenging)\n",
    "print(f\"Total records: {total_records}\")\n",
    "print(f\"% Simple: {sum(simple)/total_records}\")\n",
    "print(f\"% Moderate: {sum(moderate)/total_records}\")\n",
    "print(f\"% Challenging: {sum(challenging)/total_records}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dev_data(\"ALL\", True, \"A21\", num=10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
